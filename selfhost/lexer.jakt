// Copyright (c) 2022, JT <jt@serenityos.org>
// Copyright (c) 2022, Andreas Kling <kling@serenityos.org>
// Copyright (c) 2022, Kyle Lanmon <kyle.lanmon@gmail.com>
//
// SPDX-License-Identifier: BSD-2-Clause

import error { JaktError }
import utility { Span, is_ascii_digit, is_ascii_alpha, is_ascii_alphanumeric, is_ascii_hexdigit, is_ascii_octdigit, is_ascii_binary, is_whitespace }
import compiler { Compiler }

enum Token {
    SingleQuotedString(quote: String, prefix: String?, span: Span)
    QuotedString(quote: String, span: Span)
    Number(prefix: LiteralPrefix, number: String, suffix: LiteralSuffix, span: Span)
    Identifier(name: String, span: Span)
    Semicolon(Span)
    Colon(Span)
    ColonColon(Span)
    LParen(Span)
    RParen(Span)
    LCurly(Span)
    RCurly(Span)
    LSquare(Span)
    RSquare(Span)
    PercentSign(Span)
    Plus(Span)
    Minus(Span)
    Equal(Span)
    PlusEqual(Span)
    PlusPlus(Span)
    MinusEqual(Span)
    MinusMinus(Span)
    AsteriskEqual(Span)
    ForwardSlashEqual(Span)
    PercentSignEqual(Span)
    NotEqual(Span)
    DoubleEqual(Span)
    GreaterThan(Span)
    GreaterThanOrEqual(Span)
    LessThan(Span)
    LessThanOrEqual(Span)
    LeftArithmeticShift(Span)
    LeftShift(Span)
    LeftShiftEqual(Span)
    RightShift(Span)
    RightArithmeticShift(Span)
    RightShiftEqual(Span)
    Asterisk(Span)
    Ampersand(Span)
    AmpersandEqual(Span)
    AmpersandAmpersand(Span)
    Pipe(Span)
    PipeEqual(Span)
    PipePipe(Span)
    Caret(Span)
    CaretEqual(Span)
    Dollar(Span)
    Tilde(Span)
    ForwardSlash(Span)
    ExclamationPoint(Span)
    QuestionMark(Span)
    QuestionMarkQuestionMark(Span)
    QuestionMarkQuestionMarkEqual(Span)
    Comma(Span)
    Dot(Span)
    DotDot(Span)
    Eol(comment: String?, span: Span)
    Eof(Span)
    FatArrow(Span)
    Arrow(Span)

    // Keywords
    And(Span)
    Anon(Span)
    As(Span)
    Boxed(Span)
    Break(Span)
    Catch(Span)
    Class(Span)
    Continue(Span)
    Cpp(Span)
    Defer(Span)
    Destructor(Span)
    Else(Span)
    Enum(Span)
    Extern(Span)
    False(Span)
    For(Span)
    Fn(Span)
    Comptime(Span)
    If(Span)
    Import(Span)
    Relative(Span)
    In(Span)
    Is(Span)
    Let(Span)
    Loop(Span)
    Match(Span)
    Mut(Span)
    Namespace(Span)
    Not(Span)
    Or(Span)
    Override(Span)
    Private(Span)
    Public(Span)
    Raw(Span)
    Reflect(Span)
    Return(Span)
    Restricted(Span)
    Sizeof(Span)
    Struct(Span)
    This(Span)
    Throw(Span)
    Throws(Span)
    True(Span)
    Try(Span)
    Unsafe(Span)
    Virtual(Span)
    Weak(Span)
    While(Span)
    Yield(Span)
    Guard(Span)
    Implements(Span)
    Requires(Span)
    Trait(Span)

    // Catch-all for failed parses
    Garbage(consumed: String?, span: Span)

    public fn span(this) -> Span => match this {
        else(span) => span
    }

    fn from_keyword_or_identifier(string: String, span: Span) -> Token => match string {
        "and" => Token::And(span)
        "anon" => Token::Anon(span)
        "as" => Token::As(span)
        "boxed" => Token::Boxed(span)
        "break" => Token::Break(span)
        "catch" => Token::Catch(span)
        "class" => Token::Class(span)
        "continue" => Token::Continue(span)
        "cpp" => Token::Cpp(span)
        "defer" => Token::Defer(span)
        "destructor" => Token::Destructor(span)
        "else" => Token::Else(span)
        "enum" => Token::Enum(span)
        "extern" => Token::Extern(span)
        "false" => Token::False(span)
        "for" => Token::For(span)
        "fn" => Token::Fn(span)
        "comptime" => Token::Comptime(span)
        "if" => Token::If(span)
        "import" => Token::Import(span)
        "relative" => Token::Relative(span)
        "in" => Token::In(span)
        "is" => Token::Is(span)
        "let" => Token::Let(span)
        "loop" => Token::Loop(span)
        "match" => Token::Match(span)
        "mut" => Token::Mut(span)
        "namespace" => Token::Namespace(span)
        "not" => Token::Not(span)
        "or" => Token::Or(span)
        "override" => Token::Override(span)
        "private" => Token::Private(span)
        "public" => Token::Public(span)
        "raw" => Token::Raw(span)
        "reflect" => Token::Reflect(span)
        "return" => Token::Return(span)
        "restricted" => Token::Restricted(span)
        "sizeof" => Token::Sizeof(span)
        "struct" => Token::Struct(span)
        "this" => Token::This(span)
        "throw" => Token::Throw(span)
        "throws" => Token::Throws(span)
        "true" => Token::True(span)
        "try" => Token::Try(span)
        "unsafe" => Token::Unsafe(span)
        "virtual" => Token::Virtual(span)
        "weak" => Token::Weak(span)
        "while" => Token::While(span)
        "yield" => Token::Yield(span)
        "guard" => Token::Guard(span)
        "requires" => Token::Requires(span)
        "implements" => Token::Implements(span)
        "trait" => Token::Trait(span)
        else => Token::Identifier(name: string, span)
    }
}

enum LiteralPrefix {
    None
    Hexadecimal
    Octal
    Binary

    fn to_string(this) -> String => match this {
        None => ""
        Hexadecimal => "0x"
        Octal => "0o"
        Binary => "0b"
    }
}

enum LiteralSuffix {
    None
    UZ
    U8
    U16
    U32
    U64
    I8
    I16
    I32
    I64
    F32
    F64

    fn to_string(this) -> String => match this {
        None => ""
        UZ => "uz"
        U8 => "u8"
        U16 => "u16"
        U32 => "u32"
        U64 => "u64"
        I8 => "i8"
        I16 => "i16"
        I32 => "i32"
        I64 => "i64"
        F32 => "f32"
        F64 => "f64"
    }
}

struct Lexer implements(Iterable<Token>) {
    index: usize
    input: [u8]
    compiler: Compiler
    comment_contents: [u8]?
    illegal_cpp_keywords: {String}

    fn lex(compiler: Compiler) -> [Token] {
        let illegal_cpp_keywords: {String} = {
            "alignas",
            "alignof",
            "and_eq",
            "asm",
            "auto",
            "bitand",
            "bitor",
            "case",
            "char",
            "char8_t",
            "char16_t",
            "char32_t",
            "compl",
            "concept",
            // "const",
            "consteval",
            "constexpr",
            "constinit",
            "const_cast",
            "co_await",
            "co_return",
            "co_yield",
            "decltype",
            "delete",
            "do",
            "double",
            "dynamic_cast",
            "explicit",
            "export",
            "float",
            "friend",
            "goto",
            "int",
            "long",
            "mutable",
            "new",
            "noexcept",
            "not_eq",
            "nullptr",
            "operator",
            "or_eq",
            "protected",
            "register",
            "reinterpret_cast",
            "short",
            "signed",
            "static",
            "static_assert",
            "static_cast",
            "switch",
            "template",
            "thread_local",
            "typedef",
            "typeid",
            "typename",
            "union",
            "unsigned",
            "using",
            "volatile",
            "wchar_t",
            "xor",
            "xor_eq",
        }

        mut lexer = Lexer(index: 0, input: compiler.current_file_contents, compiler, comment_contents: None, illegal_cpp_keywords)
        mut tokens: [Token] = []

        for token in lexer {
            tokens.push(token)
        }

        return tokens
    }

    fn error(mut this, anon message: String, anon span: Span) {
        .compiler.errors.push(JaktError::Message(message, span))
    }

    fn span(this, start: usize, end: usize) -> Span {
        return Span(file_id: .compiler.current_file!, start, end)
    }

    // Peek at next upcoming character
    fn peek(this) -> u8 {
        if .eof() {
            return 0
        }
        return .input[.index]
    }

    // Peek at upcoming characters, N steps ahead in the stream
    // FIXME: This could be merged with peek() once we support default arguments
    fn peek_ahead(this, anon steps: usize) -> u8 {
        if .index + steps >= .input.size() {
            return 0
        }
        return .input[.index + steps]
    }

    fn peek_behind(this, anon steps: usize) -> u8 {
        if .index < steps {
            return 0
        }
        return .input[.index - steps]
    }

    fn eof(this) -> bool {
        return .index >= .input.size()
    }

    fn substring(this, start: usize, length: usize) -> String {
        mut builder = StringBuilder::create()
        for i in start..length {
            builder.append(.input[i])
        }
        return builder.to_string()
    }

    fn lex_character_constant_or_name(mut this) -> Token {
        if .peek_ahead(1) != b'\'' {
            return .lex_number_or_name()
        }

        let prefix: String? = match .peek() {
            b'b' => "b"
            b'c' => "c"
            else => None
        }

        if prefix.has_value() {
            .index += 1
        }

        let start = .index
        .index++

        mut escaped = false;

        while not .eof() and (escaped or .peek() != b'\'') {
            if escaped and (.index - start > 3) {
                break
            } else if .index - start > 2 {
                break
            }

            if not escaped and .peek() == b'\\' {
                escaped = true
            }

            .index++
        }

        if .eof() or .peek() != b'\'' {
            .error("Expected single quote", .span(start, end: start))
        }
        .index += 1

        // Everything but the quotes
        mut builder = StringBuilder::create()

        builder.append(.input[start + 1])
        if escaped {
            builder.append(.input[start + 2])
        }

        let quote = builder.to_string()
        let end = .index
        return Token::SingleQuotedString(quote, prefix, span: .span(start, end))
    }

    fn lex_number_or_name(mut this) -> Token {
        let start = .index

        if .eof() {
            .error("unexpected eof", .span(start, end: start))
            return Token::Garbage(consumed: None, span: .span(start, end: start))
        }

        if is_ascii_digit(.peek()) {
            return .lex_number()
        } else if is_ascii_alpha(.peek()) or .peek() == b'_' {
            mut string_builder = StringBuilder::create()

            while is_ascii_alphanumeric(.peek()) or .peek() == b'_' {
                let value = .input[.index]
                ++.index
                string_builder.append(value)
            }
            let end = .index
            let span = .span(start, end)
            let string = string_builder.to_string()

            if end - start >= 6 and string.substring(start: 0, length: 6) == "__jakt" {
                .error("reserved identifier name", span)
            }

            if .illegal_cpp_keywords.contains(string) {
                .error("C++ keywords are not allowed to be used as identifiers", span)
            }

            return Token::from_keyword_or_identifier(string, span)
        }

        let unknown_char = .input[.index]
        let end = ++.index
        .error(format("unknown character: {:c}", unknown_char), .span(start, end))
        return Token::Garbage(consumed: format("{:c}", unknown_char), span: .span(start, end))
    }

    fn valid_digit(mut this, prefix: LiteralPrefix, digit: u8, decimal_allowed: bool = true) -> bool => match prefix {
        Hexadecimal => is_ascii_hexdigit(digit)
        Octal => is_ascii_octdigit(digit)
        Binary => is_ascii_binary(digit)
        else => is_ascii_digit(digit) or (decimal_allowed and digit == b'.')
    }

    fn lex_number(mut this) -> Token {
        let start = .index

        mut floating: bool = false
        mut prefix = LiteralPrefix::None
        mut number = StringBuilder::create()

        if .peek() == b'0' {
            match .peek_ahead(1) {
                b'x' => {
                    prefix = LiteralPrefix::Hexadecimal
                    .index += 2
                }
                b'o' => {
                    prefix = LiteralPrefix::Octal
                    .index += 2
                }
                b'b' => {
                    prefix = LiteralPrefix::Binary
                    .index += 2
                } else => {}
            }
        }

        while not .eof() {
            let value = .input[.index]

            if not .valid_digit(prefix, digit: value) {
                break
            }

            if value == b'.' {
                if floating or not .valid_digit(prefix, digit: .peek_ahead(1), decimal_allowed: false) {
                    break
                }

                number.append(b'.')
                floating = true
                .index++
                continue
            }

            number.append(value)

            ++.index
            if .peek() == b'_' {
                number.append(b'_')

                if .valid_digit(prefix, digit: .peek_ahead(1)) {
                    ++.index
                } else {
                    break
                }
            }
        }

        let suffix = .consume_numeric_literal_suffix()

        return Token::Number(prefix, number: number.to_string(), suffix, span: .span(start, end: .index))
    }

    fn consume_numeric_literal_suffix(mut this) -> LiteralSuffix {
        match .peek() {
            b'u' | b'i' | b'f' => {}
            else => {
                return LiteralSuffix::None
            }
        }

        if .peek() == b'u' and .peek_ahead(1) == b'z' {
            .index += 2
            return LiteralSuffix::UZ
        }

        mut local_index = 1uz
        mut width = 0i64

        while is_ascii_digit(.peek_ahead(local_index)) {
            // Make sure we don't overflow the width
            if local_index > 3 {
                return LiteralSuffix::None
            }

            let value = .input[.index + local_index]
            ++local_index
            let digit: i64 = as_saturated(value - b'0')
            width = width * 10 + digit
        }

        let suffix = match .peek() {
            b'u' => match width {
                8 => LiteralSuffix::U8
                16 => LiteralSuffix::U16
                32 => LiteralSuffix::U32
                64 => LiteralSuffix::U64
                else => LiteralSuffix::None
            }
            b'i' => match width {
                8 => LiteralSuffix::I8
                16 => LiteralSuffix::I16
                32 => LiteralSuffix::I32
                64 => LiteralSuffix::I64
                else => LiteralSuffix::None
            }
            b'f' => match width {
                32 => LiteralSuffix::F32
                64 => LiteralSuffix::F64
                else => LiteralSuffix::None
            }
            else => LiteralSuffix::None
        }

        if not suffix is None {
            .index += local_index
        }

        return suffix
    }

    fn lex_quoted_string(mut this, delimiter: u8) -> Token {
        let start = .index

        ++.index

        if .eof() {
            .error("unexpected eof", .span(start, end: start))
            return Token::Garbage(consumed: None, span: .span(start, end: start))
        }

        mut escaped = false

        while not .eof() and (escaped or .peek() != delimiter) {
            // Ignore a standalone carriage return
            if .peek() == b'\r' or .peek() == b'\n' {
                ++.index
                continue;
            }

            if not escaped and .peek() == b'\\' {
                escaped = true
            } else {
                escaped = false
            }
            ++.index
        }

        let str = .substring(start: start + 1, length: .index)

        .index++
        let end = .index

        if delimiter == b'\'' {
            return Token::SingleQuotedString(quote: str, prefix: None, span: .span(start, end))
        }

        return Token::QuotedString(quote: str, span: .span(start, end))
    }

    fn lex_plus(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PlusEqual(.span(start, end: ++.index))
            b'+' => Token::PlusPlus(.span(start, end: ++.index))
            else => Token::Plus(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_minus(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::MinusEqual(.span(start, end: ++.index))
            b'-' => Token::MinusMinus(.span(start, end: ++.index))
            b'>' => Token::Arrow(.span(start, end: ++.index))
            else => Token::Minus(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_asterisk(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::AsteriskEqual(.span(start, end: ++.index))
            else => Token::Asterisk(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_forward_slash(mut this) -> Token {
        let start = .index++
        if .peek() == b'=' {
            return Token::ForwardSlashEqual(.span(start, end: ++.index))
        }
        if .peek() != b'/' {
            return Token::ForwardSlash(.span(start, end: .index))
        }

        if .comment_contents.has_value() {
            .index--
            return Token::Eol(
                comment: .consume_comment_contents()
                span: .span(start, end: .index)
            )
        }

        // We're in a comment, swallow to end of line.
        .index++
        let comment_start_index = .index
        while not .eof() {
            let c = .peek()
            .index++
            if c == b'\n' {
                .index--
                break
            }
        }
        .comment_contents = .input[comment_start_index...index].to_array()
        return .next() ?? Token::Eof(.span(start: .index, end: .index))
    }

    fn lex_caret(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::CaretEqual(.span(start, end: ++.index))
            else => Token::Caret(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_pipe(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PipeEqual(.span(start, end: ++.index))
            b'|' => Token::PipePipe(.span(start, end: ++.index))
            else => Token::Pipe(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_percent_sign(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PercentSignEqual(.span(start, end: ++.index))
            else => Token::PercentSign(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_exclamation_point(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::NotEqual(.span(start, end: ++.index))
            else => Token::ExclamationPoint(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_ampersand(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::AmpersandEqual(.span(start, end: ++.index))
            b'&' => Token::AmpersandAmpersand(.span(start, end: ++.index))
            else => Token::Ampersand(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_less_than(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::LessThanOrEqual(.span(start, end: ++.index))
            b'<' => {
                .index++
                yield match .peek() {
                    b'<' => Token::LeftArithmeticShift(.span(start, end: ++.index))
                    b'=' => Token::LeftShiftEqual(.span(start, end: ++.index))
                    else => Token::LeftShift(.span(start: .index - 1, end: .index))
                }
            }
            else => Token::LessThan(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_greater_than(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::GreaterThanOrEqual(.span(start, end: ++.index))
            b'>' => {
                .index++
                yield match .peek() {
                    b'>' => Token::RightArithmeticShift(.span(start, end: ++.index))
                    b'=' => Token::RightShiftEqual(.span(start, end: ++.index))
                    else => Token::RightShift(.span(start: .index - 1, end: .index))
                }
            }
            else => Token::GreaterThan(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_dot(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'.' => Token::DotDot(.span(start, end: ++.index))
            else => Token::Dot(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_colon(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b':' => Token::ColonColon(.span(start, end: ++.index))
            else => Token::Colon(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_question_mark(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'?' => {
                .index++
                yield match .peek() {
                    b'=' => Token::QuestionMarkQuestionMarkEqual(.span(start, end: ++.index))
                    else => Token::QuestionMarkQuestionMark(.span(start, end: .index))
                }
            }
            else => Token::QuestionMark(.span(start: .index - 1, end: .index))
        }
    }

    fn lex_equals(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::DoubleEqual(.span(start, end: ++.index))
            b'>' => Token::FatArrow(.span(start, end: ++.index))
            else => Token::Equal(.span(start: .index - 1, end: .index))
        }
    }

    fn consume_comment_contents(mut this) -> String? {
        if not .comment_contents.has_value() {
            return None
        }

        let contents = .comment_contents!
        .comment_contents = None
        mut builder = StringBuilder::create()
        for c in contents {
            builder.append(c)
        }

        return builder.to_string()
    }

    fn next(mut this) -> Token? {
        // Consume whitespace until a character is encountered or Eof is
        // reached. For Eof return a token.
        loop {
            if .index == .input.size() {
                ++.index
                return Token::Eof(.span(start: .index - 1, end: .index - 1))
            }
            // FIXME: Once the handling of Token::Eof is fully implemented,
            //        remove the test of eof() and return of None. The purpose
            //        of it seems to be to catch situations where index has
            //        been incremented more than one past the end of the data stream.
            if .eof() {
                return None
            }
            let ch = .peek()
            if is_whitespace(ch) {
                .index++
            } else {
                break
            }
        }

        let start = .index

        return match .input[.index] {
            b'(' => Token::LParen(.span(start, end: ++.index))
            b')' => Token::RParen(.span(start, end: ++.index))
            b'[' => Token::LSquare(.span(start, end: ++.index))
            b']' => Token::RSquare(.span(start, end: ++.index))
            b'{' => Token::LCurly(.span(start, end: ++.index))
            b'}' => Token::RCurly(.span(start, end: ++.index))
            b'<' => .lex_less_than()
            b'>' => .lex_greater_than()
            b'.' => .lex_dot()
            b',' => Token::Comma(.span(start, end: ++.index))
            b'~' => Token::Tilde(.span(start, end: ++.index))
            b';' => Token::Semicolon(.span(start, end: ++.index))
            b':' => .lex_colon()
            b'?' => .lex_question_mark()
            b'+' => .lex_plus()
            b'-' => .lex_minus()
            b'*' => .lex_asterisk()
            b'/' => .lex_forward_slash()
            b'^' => .lex_caret()
            b'|' => .lex_pipe()
            b'%' => .lex_percent_sign()
            b'!' => .lex_exclamation_point()
            b'&' => .lex_ampersand()
            b'$' => Token::Dollar(.span(start, end: ++.index))
            b'=' => .lex_equals()
            b'\n' => Token::Eol(comment: .consume_comment_contents(), span: .span(start, end: ++.index))
            b'\'' => .lex_quoted_string(delimiter: b'\'')
            b'\"' => .lex_quoted_string(delimiter: b'"')
            b'b' => .lex_character_constant_or_name()
            b'c' => .lex_character_constant_or_name()
            else => .lex_number_or_name()
        }
    }
}
