// Copyright (c) 2022, JT <jt@serenityos.org>
// Copyright (c) 2022, Andreas Kling <kling@serenityos.org>
// Copyright (c) 2022, Kyle Lanmon <kyle.lanmon@gmail.com>
//
// SPDX-License-Identifier: BSD-2-Clause

import error { JaktError }
import utility { Span, is_ascii_digit, is_ascii_alpha, is_ascii_alphanumeric, is_ascii_hexdigit, is_ascii_octdigit, is_ascii_binary, is_whitespace }
import compiler { Compiler }

enum Token {
    span: Span
    SingleQuotedString(quote: String, prefix: String?)
    QuotedString(quote: String)
    Number(prefix: LiteralPrefix, number: String, suffix: LiteralSuffix)
    Identifier(name: String)
    Semicolon
    Colon
    ColonColon
    LParen
    RParen
    LCurly
    RCurly
    LSquare
    RSquare
    PercentSign
    Plus
    Minus
    Equal
    PlusEqual
    PlusPlus
    MinusEqual
    MinusMinus
    AsteriskEqual
    ForwardSlashEqual
    PercentSignEqual
    NotEqual
    DoubleEqual
    GreaterThan
    GreaterThanOrEqual
    LessThan
    LessThanOrEqual
    LeftArithmeticShift
    LeftShift
    LeftShiftEqual
    RightShift
    RightArithmeticShift
    RightShiftEqual
    Asterisk
    Ampersand
    AmpersandEqual
    AmpersandAmpersand
    Pipe
    PipeEqual
    PipePipe
    Caret
    CaretEqual
    Dollar
    Tilde
    ForwardSlash
    ExclamationPoint
    QuestionMark
    QuestionMarkQuestionMark
    QuestionMarkQuestionMarkEqual
    Comma
    Dot
    DotDot
    Eol(comment: String?)
    Eof
    FatArrow
    Arrow

    // Keywords
    And
    Anon
    As
    Boxed
    Break
    Catch
    Class
    Continue
    Cpp
    Defer
    Destructor
    Else
    Enum
    Extern
    False
    For
    Fn
    Comptime
    If
    Import
    Relative
    In
    Is
    Let
    Loop
    Match
    Must
    Mut
    Namespace
    Not
    Or
    Override
    Private
    Public
    Raw
    Reflect
    Return
    Restricted
    Sizeof
    Struct
    This
    Throw
    Throws
    True
    Try
    Unsafe
    Virtual
    Weak
    While
    Yield
    Guard
    Implements
    Requires
    Trait

    // Catch-all for failed parses
    Garbage(consumed: String?)

    fn from_keyword_or_identifier(string: String, span: Span) -> Token => match string {
        "and" => Token::And(span)
        "anon" => Token::Anon(span)
        "as" => Token::As(span)
        "boxed" => Token::Boxed(span)
        "break" => Token::Break(span)
        "catch" => Token::Catch(span)
        "class" => Token::Class(span)
        "continue" => Token::Continue(span)
        "cpp" => Token::Cpp(span)
        "defer" => Token::Defer(span)
        "destructor" => Token::Destructor(span)
        "else" => Token::Else(span)
        "enum" => Token::Enum(span)
        "extern" => Token::Extern(span)
        "false" => Token::False(span)
        "for" => Token::For(span)
        "fn" => Token::Fn(span)
        "comptime" => Token::Comptime(span)
        "if" => Token::If(span)
        "import" => Token::Import(span)
        "relative" => Token::Relative(span)
        "in" => Token::In(span)
        "is" => Token::Is(span)
        "let" => Token::Let(span)
        "loop" => Token::Loop(span)
        "match" => Token::Match(span)
        "must" => Token::Must(span)
        "mut" => Token::Mut(span)
        "namespace" => Token::Namespace(span)
        "not" => Token::Not(span)
        "or" => Token::Or(span)
        "override" => Token::Override(span)
        "private" => Token::Private(span)
        "public" => Token::Public(span)
        "raw" => Token::Raw(span)
        "reflect" => Token::Reflect(span)
        "return" => Token::Return(span)
        "restricted" => Token::Restricted(span)
        "sizeof" => Token::Sizeof(span)
        "struct" => Token::Struct(span)
        "this" => Token::This(span)
        "throw" => Token::Throw(span)
        "throws" => Token::Throws(span)
        "true" => Token::True(span)
        "try" => Token::Try(span)
        "unsafe" => Token::Unsafe(span)
        "virtual" => Token::Virtual(span)
        "weak" => Token::Weak(span)
        "while" => Token::While(span)
        "yield" => Token::Yield(span)
        "guard" => Token::Guard(span)
        "requires" => Token::Requires(span)
        "implements" => Token::Implements(span)
        "trait" => Token::Trait(span)
        else => Token::Identifier(span, name: string)
    }
}

enum LiteralPrefix {
    None
    Hexadecimal
    Octal
    Binary

    fn to_string(this) -> String => match this {
        None => ""
        Hexadecimal => "0x"
        Octal => "0o"
        Binary => "0b"
    }
}

enum LiteralSuffix {
    None
    UZ
    U8
    U16
    U32
    U64
    I8
    I16
    I32
    I64
    F32
    F64

    fn to_string(this) -> String => match this {
        None => ""
        UZ => "uz"
        U8 => "u8"
        U16 => "u16"
        U32 => "u32"
        U64 => "u64"
        I8 => "i8"
        I16 => "i16"
        I32 => "i32"
        I64 => "i64"
        F32 => "f32"
        F64 => "f64"
    }
}

struct Lexer implements(Iterable<Token>) {
    index: usize
    input: [u8]
    compiler: Compiler
    comment_contents: [u8]?
    illegal_cpp_keywords: {String}

    fn lex(compiler: Compiler) -> [Token] {
        let illegal_cpp_keywords: {String} = {
            "alignas",
            "alignof",
            "and_eq",
            "asm",
            "auto",
            "bitand",
            "bitor",
            "case",
            "char",
            "char8_t",
            "char16_t",
            "char32_t",
            "compl",
            "concept",
            // "const",
            "consteval",
            "constexpr",
            "constinit",
            "const_cast",
            "co_await",
            "co_return",
            "co_yield",
            "decltype",
            "delete",
            "do",
            "double",
            "dynamic_cast",
            "explicit",
            "export",
            "float",
            "friend",
            "goto",
            "int",
            "long",
            "mutable",
            "new",
            "noexcept",
            "not_eq",
            "nullptr",
            "operator",
            "or_eq",
            "protected",
            "register",
            "reinterpret_cast",
            "short",
            "signed",
            "static",
            "static_assert",
            "static_cast",
            "switch",
            "template",
            "thread_local",
            "typedef",
            "typeid",
            "typename",
            "union",
            "unsigned",
            "using",
            "volatile",
            "wchar_t",
            "xor",
            "xor_eq",
        }

        mut lexer = Lexer(index: 0, input: compiler.current_file_contents, compiler, comment_contents: None, illegal_cpp_keywords)
        mut tokens: [Token] = []

        for token in lexer {
            tokens.push(token)
        }

        return tokens
    }

    fn error(mut this, anon message: String, anon span: Span) {
        .compiler.errors.push(JaktError::Message(message, span))
    }

    fn span(this, start: usize, end: usize) -> Span {
        return Span(file_id: .compiler.current_file!, start, end)
    }

    // Peek at next upcoming character
    fn peek(this) -> u8 {
        if .eof() {
            return 0
        }
        return .input[.index]
    }

    // Peek at upcoming characters, N steps ahead in the stream
    // FIXME: This could be merged with peek() once we support default arguments
    fn peek_ahead(this, anon steps: usize) -> u8 {
        if .index + steps >= .input.size() {
            return 0
        }
        return .input[.index + steps]
    }

    fn peek_behind(this, anon steps: usize) -> u8 {
        if .index < steps {
            return 0
        }
        return .input[.index - steps]
    }

    fn eof(this) -> bool {
        return .index >= .input.size()
    }

    fn substring(this, start: usize, length: usize) -> String {
        mut builder = StringBuilder::create()
        for i in start..length {
            builder.append(.input[i])
        }
        return builder.to_string()
    }

    fn lex_character_constant_or_name(mut this) -> Token {
        if .peek_ahead(1) != b'\'' {
            return .lex_number_or_name()
        }

        let prefix: String? = match .peek() {
            b'b' => "b"
            b'c' => "c"
            else => None
        }

        if prefix.has_value() {
            .index += 1
        }

        let start = .index
        .index++

        mut escaped = false;

        while not .eof() and (escaped or .peek() != b'\'') {
            if escaped and (.index - start > 3) {
                break
            } else if .index - start > 2 {
                break
            }

            if not escaped and .peek() == b'\\' {
                escaped = true
            }

            .index++
        }

        if .eof() or .peek() != b'\'' {
            .error("Expected single quote", .span(start, end: start))
        }
        .index += 1

        // Everything but the quotes
        mut builder = StringBuilder::create()

        builder.append(.input[start + 1])
        if escaped {
            builder.append(.input[start + 2])
        }

        let quote = builder.to_string()
        let end = .index
        return Token::SingleQuotedString(span: .span(start, end), quote, prefix)
    }

    fn lex_number_or_name(mut this) -> Token {
        let start = .index

        if .eof() {
            .error("unexpected eof", .span(start, end: start))
            return Token::Garbage(span: .span(start, end: start), consumed: None)
        }

        if is_ascii_digit(.peek()) {
            return .lex_number()
        } else if is_ascii_alpha(.peek()) or .peek() == b'_' {
            mut string_builder = StringBuilder::create()

            while is_ascii_alphanumeric(.peek()) or .peek() == b'_' {
                let value = .input[.index]
                ++.index
                string_builder.append(value)
            }
            let end = .index
            let span = .span(start, end)
            let string = string_builder.to_string()

            if end - start >= 6 and string.substring(start: 0, length: 6) == "__jakt" {
                .error("reserved identifier name", span)
            }

            if .illegal_cpp_keywords.contains(string) {
                .error("C++ keywords are not allowed to be used as identifiers", span)
            }

            return Token::from_keyword_or_identifier(string, span)
        }

        let unknown_char = .input[.index]
        let end = ++.index
        .error(format("unknown character: {:c}", unknown_char), .span(start, end))
        return Token::Garbage(span: .span(start, end),consumed: format("{:c}", unknown_char))
    }

    fn valid_digit(mut this, prefix: LiteralPrefix, digit: u8, decimal_allowed: bool = true) -> bool => match prefix {
        Hexadecimal => is_ascii_hexdigit(digit)
        Octal => is_ascii_octdigit(digit)
        Binary => is_ascii_binary(digit)
        else => is_ascii_digit(digit) or (decimal_allowed and digit == b'.')
    }

    fn lex_number(mut this) -> Token {
        let start = .index

        mut floating: bool = false
        mut prefix = LiteralPrefix::None
        mut number = StringBuilder::create()

        if .peek() == b'0' {
            match .peek_ahead(1) {
                b'x' => {
                    prefix = LiteralPrefix::Hexadecimal
                    .index += 2
                }
                b'o' => {
                    prefix = LiteralPrefix::Octal
                    .index += 2
                }
                b'b' => {
                    prefix = LiteralPrefix::Binary
                    .index += 2
                } else => {}
            }
        }

        while not .eof() {
            let value = .input[.index]

            if not .valid_digit(prefix, digit: value) {
                break
            }

            if value == b'.' {
                if floating or not .valid_digit(prefix, digit: .peek_ahead(1), decimal_allowed: false) {
                    break
                }

                number.append(b'.')
                floating = true
                .index++
                continue
            }

            number.append(value)

            ++.index
            if .peek() == b'_' {
                number.append(b'_')

                if .valid_digit(prefix, digit: .peek_ahead(1)) {
                    ++.index
                } else {
                    break
                }
            }
        }

        let suffix = .consume_numeric_literal_suffix()

        return Token::Number(span: .span(start, end: .index), prefix, number: number.to_string(), suffix)
    }

    fn consume_numeric_literal_suffix(mut this) -> LiteralSuffix {
        match .peek() {
            b'u' | b'i' | b'f' => {}
            else => {
                return LiteralSuffix::None
            }
        }

        if .peek() == b'u' and .peek_ahead(1) == b'z' {
            .index += 2
            return LiteralSuffix::UZ
        }

        mut local_index = 1uz
        mut width = 0i64

        while is_ascii_digit(.peek_ahead(local_index)) {
            // Make sure we don't overflow the width
            if local_index > 3 {
                return LiteralSuffix::None
            }

            let value = .input[.index + local_index]
            ++local_index
            let digit: i64 = as_saturated(value - b'0')
            width = width * 10 + digit
        }

        let suffix = match .peek() {
            b'u' => match width {
                8 => LiteralSuffix::U8
                16 => LiteralSuffix::U16
                32 => LiteralSuffix::U32
                64 => LiteralSuffix::U64
                else => LiteralSuffix::None
            }
            b'i' => match width {
                8 => LiteralSuffix::I8
                16 => LiteralSuffix::I16
                32 => LiteralSuffix::I32
                64 => LiteralSuffix::I64
                else => LiteralSuffix::None
            }
            b'f' => match width {
                32 => LiteralSuffix::F32
                64 => LiteralSuffix::F64
                else => LiteralSuffix::None
            }
            else => LiteralSuffix::None
        }

        if not suffix is None {
            .index += local_index
        }

        return suffix
    }

    fn lex_quoted_string(mut this, delimiter: u8) -> Token {
        let start = .index

        ++.index

        if .eof() {
            .error("unexpected eof", .span(start, end: start))
            return Token::Garbage(span: .span(start, end: start), consumed: None)
        }

        mut escaped = false

        while not .eof() and (escaped or .peek() != delimiter) {
            // Ignore a standalone carriage return
            if .peek() == b'\r' or .peek() == b'\n' {
                ++.index
                continue;
            }

            if not escaped and .peek() == b'\\' {
                escaped = true
            } else {
                escaped = false
            }
            ++.index
        }

        let str = .substring(start: start + 1, length: .index)

        .index++
        let end = .index

        if delimiter == b'\'' {
            return Token::SingleQuotedString(span: .span(start, end), quote: str, prefix: None)
        }

        return Token::QuotedString(span: .span(start, end), quote: str)
    }

    fn lex_plus(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PlusEqual(span: .span(start, end: ++.index))
            b'+' => Token::PlusPlus(span: .span(start, end: ++.index))
            else => Token::Plus(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_minus(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::MinusEqual(span: .span(start, end: ++.index))
            b'-' => Token::MinusMinus(span: .span(start, end: ++.index))
            b'>' => Token::Arrow(span: .span(start, end: ++.index))
            else => Token::Minus(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_asterisk(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::AsteriskEqual(span: .span(start, end: ++.index))
            else => Token::Asterisk(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_forward_slash(mut this) -> Token {
        let start = .index++
        if .peek() == b'=' {
            return Token::ForwardSlashEqual(span: .span(start, end: ++.index))
        }
        if .peek() != b'/' {
            return Token::ForwardSlash(span: .span(start, end: .index))
        }

        if .comment_contents.has_value() {
            .index--
            return Token::Eol(
                span: .span(start, end: .index)
                comment: .consume_comment_contents()
            )
        }

        // We're in a comment, swallow to end of line.
        .index++
        let comment_start_index = .index
        while not .eof() {
            let c = .peek()
            .index++
            if c == b'\n' {
                .index--
                break
            }
        }
        .comment_contents = .input[comment_start_index...index].to_array()
        return .next() ?? Token::Eof(span: .span(start: .index, end: .index))
    }

    fn lex_caret(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::CaretEqual(span: .span(start, end: ++.index))
            else => Token::Caret(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_pipe(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PipeEqual(span: .span(start, end: ++.index))
            b'|' => Token::PipePipe(span: .span(start, end: ++.index))
            else => Token::Pipe(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_percent_sign(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::PercentSignEqual(span: .span(start, end: ++.index))
            else => Token::PercentSign(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_exclamation_point(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::NotEqual(span: .span(start, end: ++.index))
            else => Token::ExclamationPoint(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_ampersand(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::AmpersandEqual(span: .span(start, end: ++.index))
            b'&' => Token::AmpersandAmpersand(span: .span(start, end: ++.index))
            else => Token::Ampersand(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_less_than(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::LessThanOrEqual(span: .span(start, end: ++.index))
            b'<' => {
                .index++
                yield match .peek() {
                    b'<' => Token::LeftArithmeticShift(span: .span(start, end: ++.index))
                    b'=' => Token::LeftShiftEqual(span: .span(start, end: ++.index))
                    else => Token::LeftShift(span: .span(start: .index - 1, end: .index))
                }
            }
            else => Token::LessThan(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_greater_than(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::GreaterThanOrEqual(span: .span(start, end: ++.index))
            b'>' => {
                .index++
                yield match .peek() {
                    b'>' => Token::RightArithmeticShift(span: .span(start, end: ++.index))
                    b'=' => Token::RightShiftEqual(span: .span(start, end: ++.index))
                    else => Token::RightShift(span: .span(start: .index - 1, end: .index))
                }
            }
            else => Token::GreaterThan(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_dot(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'.' => Token::DotDot(span: .span(start, end: ++.index))
            else => Token::Dot(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_colon(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b':' => Token::ColonColon(span: .span(start, end: ++.index))
            else => Token::Colon(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_question_mark(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'?' => {
                .index++
                yield match .peek() {
                    b'=' => Token::QuestionMarkQuestionMarkEqual(span: .span(start, end: ++.index))
                    else => Token::QuestionMarkQuestionMark(span: .span(start, end: .index))
                }
            }
            else => Token::QuestionMark(span: .span(start: .index - 1, end: .index))
        }
    }

    fn lex_equals(mut this) -> Token {
        let start = .index++
        return match .peek() {
            b'=' => Token::DoubleEqual(span: .span(start, end: ++.index))
            b'>' => Token::FatArrow(span: .span(start, end: ++.index))
            else => Token::Equal(span: .span(start: .index - 1, end: .index))
        }
    }

    fn consume_comment_contents(mut this) -> String? {
        if not .comment_contents.has_value() {
            return None
        }

        let contents = .comment_contents!
        .comment_contents = None
        mut builder = StringBuilder::create()
        for c in contents {
            builder.append(c)
        }

        return builder.to_string()
    }

    fn next(mut this) -> Token? {
        // Consume whitespace until a character is encountered or Eof is
        // reached. For Eof return a token.
        loop {
            if .index == .input.size() {
                ++.index
                return Token::Eof(span: .span(start: .index - 1, end: .index - 1))
            }
            // FIXME: Once the handling of Token::Eof is fully implemented,
            //        remove the test of eof() and return of None. The purpose
            //        of it seems to be to catch situations where index has
            //        been incremented more than one past the end of the data stream.
            if .eof() {
                return None
            }
            let ch = .peek()
            if is_whitespace(ch) {
                .index++
            } else {
                break
            }
        }

        let start = .index

        return match .input[.index] {
            b'(' => Token::LParen(span: .span(start, end: ++.index))
            b')' => Token::RParen(span: .span(start, end: ++.index))
            b'[' => Token::LSquare(span: .span(start, end: ++.index))
            b']' => Token::RSquare(span: .span(start, end: ++.index))
            b'{' => Token::LCurly(span: .span(start, end: ++.index))
            b'}' => Token::RCurly(span: .span(start, end: ++.index))
            b'<' => .lex_less_than()
            b'>' => .lex_greater_than()
            b'.' => .lex_dot()
            b',' => Token::Comma(span: .span(start, end: ++.index))
            b'~' => Token::Tilde(span: .span(start, end: ++.index))
            b';' => Token::Semicolon(span: .span(start, end: ++.index))
            b':' => .lex_colon()
            b'?' => .lex_question_mark()
            b'+' => .lex_plus()
            b'-' => .lex_minus()
            b'*' => .lex_asterisk()
            b'/' => .lex_forward_slash()
            b'^' => .lex_caret()
            b'|' => .lex_pipe()
            b'%' => .lex_percent_sign()
            b'!' => .lex_exclamation_point()
            b'&' => .lex_ampersand()
            b'$' => Token::Dollar(span: .span(start, end: ++.index))
            b'=' => .lex_equals()
            b'\n' => Token::Eol(span: .span(start, end: ++.index), comment: .consume_comment_contents())
            b'\'' => .lex_quoted_string(delimiter: b'\'')
            b'\"' => .lex_quoted_string(delimiter: b'"')
            b'b' => .lex_character_constant_or_name()
            b'c' => .lex_character_constant_or_name()
            else => .lex_number_or_name()
        }
    }
}
